# ğŸ§  AI Response Evaluation Project

This project compares two AI model outputs (A vs B) using a human feedback rubric with five criteria:
**Accuracy**, **Relevance**, **Clarity**, **Actionability**, and **Tone/Safety**.

## ğŸ“Š Features
- Annotated 12 prompts from diverse domains (IT support, cloud computing, writing, ethics)
- Python script to compute average scores and generate comparison charts
- Visual results saved in `figures/` for quick insight

## ğŸ§° Tech Used
- Python (pandas, matplotlib)
- CSV data and markdown documentation

## ğŸ§© Folder Structure
```
AI-Response-Evaluation/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ annotations.csv
â”œâ”€â”€ figures/
â”œâ”€â”€ analyze.py
â”œâ”€â”€ rubric.md
â”œâ”€â”€ report.md
â””â”€â”€ README.md
```

## ğŸš€ How to Run
```bash
pip install pandas matplotlib
python analyze.py
```

## ğŸ“ˆ Example Output
After running `analyze.py`, charts will be saved in `figures/`.

## âœï¸ Author
**Miguel Castaneda**  
- U.S. Air Force Veteran | Computer Science Student | Azure Certified (AZ-900)  
- Email: michaelhe05@gmail.com
