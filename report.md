# Evaluating AI Responses Using Human Feedback Metrics

**Overview**  
This project compares two model variants (A vs B) across 12 prompts. Each answer is rated 1–5 on Accuracy, Relevance, Clarity, Actionability, and Tone/Safety.

**Methods**  
Prompts span IT support, cloud, writing, reasoning, and safety. Scores are averaged per model. Preferred choices are counted as head‑to‑head wins.

**Results (insert figures after running `analyze.py`)**  
- See `summary.csv` for table of averages and wins.  
- Charts saved to `figures/`.

**Insights**  
- Add your 3–5 takeaways here.

**Limitations**  
Small sample, single rater. Next iteration will expand prompts, add a second rater, and include reference checks for factual items.
